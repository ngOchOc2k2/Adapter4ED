{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from model.engine import BertClassifier, RobertaClassifier\n",
    "from model.bert import BertModel\n",
    "from model.roberta import RobertaModel\n",
    "from train import DoubleLoss\n",
    "from transformers import AutoModel, BertModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Since, we want to optimize only parameters of the adapter modules and layer normalization\n",
    "# l=[\"adapter\", \"LayerNorm\"]\n",
    "# [n for n, p in model.named_parameters() if any([(nd in n) for nd in l])]\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaClassifier(model_path='roberta-base', num_labels=100, tokenizer='roberta-base')\n",
    "# model_bert = AutoModel.from_pretrained('roberta-base')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "strategy = 'adapter'\n",
    "trainable_params_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable params: 2417664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "optimizer_grouped_parameters = []\n",
    "if strategy == 'full-finetuning':\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.1,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "elif strategy == 'adapter':\n",
    "    no_decay = [\"adapter.proj_up.bias\", \"adapter.proj_down.bias\", \"LayerNorm\"]\n",
    "    cls_bias = ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
    "    cls_weight = ['cls.seq_relationship.weight', 'cls.predicions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
    "    layers = [\"adapter.proj_up.weight\", \"adapter.proj_down.weight\"]\n",
    "    layers.extend(cls_weight)\n",
    "    no_decay.extend(cls_bias)\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any([nd in n for nd in layers])],\n",
    "            \"weight_decay\": 0.1,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# count the total no. of trainable params\n",
    "for group in optimizer_grouped_parameters:\n",
    "    for param in group[\"params\"]:\n",
    "        trainable_params_count += param.numel()\n",
    "print(f'Total Trainable params: {trainable_params_count}')\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=1e-5, eps=1e-6)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=32,\n",
    ")\n",
    "scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scheduler': <torch.optim.lr_scheduler.LambdaLR at 0x7fb36780fc50>,\n",
       "  'interval': 'step',\n",
       "  'frequency': 1}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[scheduler]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
